{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation:\n",
    "# When adjusting models we are aiming to increase overall model performance on unseen data. \n",
    "# Hyperparameter tuning can lead to much better performance on test sets. However, optimizing \n",
    "# parameters to the test set can lead information leakage causing the model to preform worse on \n",
    "# unseen data. To correct for this we can perform cross validation.\n",
    "\n",
    "# To better understand CV, we will be performing different methods on the iris dataset. Let us first \n",
    "# load in and separate the data.\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# There are many methods to cross validation, we will start by looking at k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold:\n",
    "\n",
    "# The training data used in the model is split, into k number of smaller sets, to be used to \n",
    "# validate the model. The model is then trained on k-1 folds of training set. The remaining \n",
    "# fold is then used as a validation set to evaluate the model.\n",
    "\n",
    "# As we will be trying to classify different species of iris flowers we will need to import a \n",
    "# classifier model, for this exercise we will be using a DecisionTreeClassifier. We will also \n",
    "# need to import CV modules from sklearn.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# With the data loaded we can now create and fit a model for evaluation.\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Now let's evaluate our model and see how it performs on each k-fold.\n",
    "\n",
    "k_folds = KFold(n_splits = 5)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv = k_folds)\n",
    "\n",
    "# It is also good pratice to see how CV performed overall by averaging the scores for all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [1.         1.         0.83333333 0.93333333 0.8       ]\n",
      "Average CV Score:  0.9133333333333333\n",
      "Number of CV Scores used in Average:  5\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "# Run k-fold CV:\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "k_folds = KFold(n_splits = 5)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv = k_folds)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K-Fold:\n",
    "# In cases where classes are imbalanced we need a way to account for the imbalance in both the \n",
    "# train and validation sets. To do so we can stratify the target classes, meaning that both sets \n",
    "# will have an equal proportion of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.96666667 0.96666667 0.9        0.93333333 1.        ]\n",
      "Average CV Score:  0.9533333333333334\n",
      "Number of CV Scores used in Average:  5\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "sk_folds = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv = sk_folds)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While the number of folds is the same, the average CV increases from the basic k-fold when \n",
    "# making sure there is stratified classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-One-Out (LOO):\n",
    "# Instead of selecting the number of splits in the training data set like k-fold LeaveOneOut, \n",
    "# utilize 1 observation to validate and n-1 observations to train. This method is an exaustive technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "Average CV Score:  0.94\n",
      "Number of CV Scores used in Average:  150\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "# Run LOO CV:\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv = loo)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can observe that the number of cross validation scores performed is equal to the number of \n",
    "# observations in the dataset. In this case there are 150 observations in the iris dataset.\n",
    "\n",
    "# The average CV score is 94%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave-P-Out (LPO):\n",
    "# Leave-P-Out is simply a nuanced diffence to the Leave-One-Out idea, in that we can select the \n",
    "# number of p to use in our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [1. 1. 1. ... 1. 1. 1.]\n",
      "Average CV Score:  0.9382997762863534\n",
      "Number of CV Scores used in Average:  11175\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "# Run LPO CV:\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import LeavePOut, cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "lpo = LeavePOut(p=2)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv = lpo)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see this is an exhaustive method we many more scores being calculated than Leave-One-Out, \n",
    "# even with a p = 2, yet it achieves roughly the same average CV score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Split:\n",
    "# Unlike KFold, ShuffleSplit leaves out a percentage of the data, not to be used in the \n",
    "# train or validation sets. To do so we must decide what the train and test sizes are, \n",
    "# as well as the number of splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.93333333 0.93333333 0.93333333 0.97777778 0.95555556]\n",
      "Average CV Score:  0.9466666666666667\n",
      "Number of CV Scores used in Average:  5\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "# Run Shuffle Split CV:\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "ss = ShuffleSplit(train_size=0.6, test_size=0.3, n_splits = 5)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv = ss)\n",
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ending Notes:\n",
    "\n",
    "# These are just a few of the CV methods that can be applied to models. There are many more \n",
    "# cross validation classes, with most models having their own class. Check out sklearns cross \n",
    "# validation for more CV options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
